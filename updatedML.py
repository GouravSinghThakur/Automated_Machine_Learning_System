import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.utils.class_weight import compute_class_weight
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc,classification_report
from sklearn.impute import SimpleImputer
import openpyxl
from xgboost import XGBClassifier
import optuna
import joblib
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt

st.set_page_config(page_title="ML Model Deployment", layout="wide")

def load_data(file):
    try:
        if file.name.endswith('.csv'):
            data = pd.read_csv(file)
        elif file.name.endswith(('.xls', '.xlsx')):
            data = pd.read_excel(file)
        return data
    except Exception as e:
        st.error(f"Error loading file: {e}")
        return None

def auto_process_data(data):
    processed_data = data.copy()
    label_encoders = {}
    
    if processed_data.isnull().sum().sum() > 0:
        st.info("Automatically handling missing values...")
        
        num_cols = processed_data.select_dtypes(include=['int64', 'float64']).columns
        if len(num_cols) > 0:
            num_imputer = SimpleImputer(strategy='median')
            processed_data[num_cols] = num_imputer.fit_transform(processed_data[num_cols])
        
        cat_cols = processed_data.select_dtypes(include=['object']).columns
        if len(cat_cols) > 0:
            for col in cat_cols:
                if processed_data[col].isnull().any():
                    most_frequent = processed_data[col].mode()[0]
                    processed_data[col].fillna(most_frequent, inplace=True)
    
    for column in processed_data.select_dtypes(include=['object']):
        label_encoders[column] = LabelEncoder()
        processed_data[column] = label_encoders[column].fit_transform(processed_data[column].astype(str))
    
    return processed_data, label_encoders

def get_model_configs():
    models = {
        'Logistic Regression': {
            'pipeline': Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', LogisticRegression())
            ]),
            'params': {
                'classifier__penalty':['l1','l2'],
                'classifier__C':[0.01,0.1,1],
                'classifier__max_iter': [100, 200],
                'classifier__solver':['liblinear','saga']
            }
        },
        'Support Vector Machine': {
            'pipeline': Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', SVC(probability=True))
            ]),
            'params': {
                'classifier__C': [0.001, 0.1, 1],
                'classifier__kernel': ['linear', 'rbf', 'sigmoid'],
                'classifier__gamma': ['scale', 'auto', 0.01, 0.1, 1],
                'classifier__max_iter':[100,200]
            }
        },
        'Random Forest': {
            'pipeline': Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier())
            ]),
            'params': {
                'classifier__n_estimators':[100,200],
                'classifier__max_depth': [None, 10, 20],
                'classifier__min_samples_split': [2,5,10],
                'classifier__min_samples_leaf':[1,2,4],
            }
        },
        'XgBoost':{
            'pipeline':Pipeline([
            ('scaled',StandardScaler()),
            ('classifier',XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
            ]),
            'params':{
                'classifier__n_estimators':[100,200],
                'classifier__n_estimators': [100, 200],
                'classifier__learning_rate': [0.01, 0.05, 0.1],
                'classifier__max_depth': [3, 5, 7],
                'classifier__gamma': [0, 0.1, 0.2],
                'classifier__min_child_weight': [1, 3, 5],
                'classifier__subsample': [0.8, 1.0],
                'classifier__colsample_bytree': [0.8, 1.0],
                'classifier__reg_lambda': [1, 1.5],
                'classifier__reg_alpha': [0, 0.5]
            }
        }
    }
    return models

def train_model(X_train, y_train, selected_model, progress_bar=None):
    models = get_model_configs()
    model_config = models[selected_model]
    
    with st.spinner(f"Training {selected_model}..."):
        grid_search = GridSearchCV(
            estimator=model_config['pipeline'],
            param_grid=model_config['params'],
            cv=2,
            n_jobs=-1,
            verbose=0,
            scoring="accuracy"
        )
        grid_search.fit(X_train, y_train)
        
        if progress_bar:
            progress_bar.progress(1.0)
        
        return grid_search.best_estimator_, grid_search.best_score_
def objective(trial, X_train, y_train, model_name):
    models = get_model_configs()
    model_config = models[model_name]
    dataset_size=len(X_train)
    if dataset_size > 1000:
        cv_folds = 5
    elif dataset_size > 500:
        cv_folds = 3
    else:
        cv_folds = min(2, dataset_size)
    unique_classes,count_classes=np.unique(X_train,return_counts=True)
    class_weights=compute_class_weight(class_weight='balaned',classes=unique_classes,y=y_train)
    imbalance_ratio = max(count_classes)/min(count_classes)
    if imbalance_ratio >1.5:
        scoring_metric = 'f1_weighted'
    else:
        scoring_metric = 'accuracy'
    
    if model_name == 'Logistic Regression':
        penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
        C = trial.suggest_float('C', 0.01, 1.0, log=True)
        solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])
        max_iter = trial.suggest_int('max_iter', 100, 200)
        params = {'classifier__penalty': penalty,
            'classifier__C': C,
            'classifier__solver': solver,
            'classifier__max_iter': max_iter}c
        C = trial.suggest_float('C', 0.001, 1.0, log=True)
        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid'])
        gamma = trial.suggest_categorical('gamma', ['scale', 'auto', 0.01, 0.1, 1])
        max_iter = trial.suggest_int('max_iter', 100, 200)
        params = {'classifier__C': C,
            'classifier__kernel': kernel,
            'classifier__gamma': gamma,
            'classifier__max_iter': max_iter}
    
    elif model_name == 'Random Forest':
        n_estimators = trial.suggest_int('n_estimators', 100, 200)
        max_depth = trial.suggest_categorical('max_depth', [None, 10, 20])
        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)
        params = {'classifier__n_estimators': n_estimators,
            'classifier__max_depth': max_depth,
            'classifier__min_samples_split': min_samples_split,
            'classifier__min_samples_leaf': min_samples_leaf}
    elif model_name == 'XGBoost':
        n_estimators = trial.suggest_int('n_estimators', 100, 300)
        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)
        max_depth = trial.suggest_int('max_depth', 3, 10)
        gamma = trial.suggest_float('gamma', 0.0, 0.3)
        min_child_weight = trial.suggest_int('min_child_weight', 1, 6)
        subsample = trial.suggest_float('subsample', 0.5, 1.0)
        colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)
        reg_lambda = trial.suggest_float('reg_lambda', 0.1, 2.0)
        reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)

        params = {
            'classifier__n_estimators': n_estimators,
            'classifier__learning_rate': learning_rate,
            'classifier__max_depth': max_depth,
            'classifier__gamma': gamma,
            'classifier__min_child_weight': min_child_weight,
            'classifier__subsample': subsample,
            'classifier__colsample_bytree': colsample_bytree,
            'classifier__reg_lambda': reg_lambda,
            'classifier__reg_alpha': reg_alpha
        }

    pipeline = model_config['pipeline'].set_params(**params)
    Pipeline.fit(X_train,y_train)
    score=cross_val_score(pipeline,X_train,y_train,cv=cv_folds,scoring=scoring_metric).mean()
    return score

def train_model_with_optuna(X_train, y_train, selected_model):
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, X_train, y_train, selected_model), n_trials=20)
    
    best_params = study.best_params
    models = get_model_configs()
    best_model = models[selected_model]['pipeline'].set_params(**best_params)
    best_model.fit(X_train, y_train)
    
    return best_model, study.best_value

def auto_train(X_train, y_train, X_test, y_test):
    models = get_model_configs()
    results = {}
    best_score = 0
    best_model = None
    best_model_name = None
    
    st.write("Training multiple models to find the best one...")
    
    progress_cols = st.columns(len(models))
    progress_bars = {}
    
    for i, (model_name, _) in enumerate(models.items()):
        with progress_cols[i]:
            st.write(f"Training {model_name}")
            progress_bars[model_name] = st.progress(0.0)
    
    for model_name in models.keys():
        trained_model, cv_score = train_model(X_train, y_train, model_name, progress_bars[model_name])
        
        y_pred = trained_model.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_pred)
        
        results[model_name] = {
            'model': trained_model,
            'cv_score': cv_score,
            'test_accuracy': test_accuracy
        }
        
        if test_accuracy > best_score:
            best_score = test_accuracy
            best_model = trained_model
            best_model_name = model_name
    
    results_df = pd.DataFrame({
        'Model': list(results.keys()),
        'Cross-Validation Score': [results[model]['cv_score'] for model in results],
        'Test Accuracy': [results[model]['test_accuracy'] for model in results]
    })
    
    st.subheader("Model Comparison")
    st.dataframe(results_df.sort_values('Test Accuracy', ascending=False))
    
    st.success(f"Best model: {best_model_name} with accuracy: {best_score:.2%}")
    
    return best_model, best_model_name

def get_classification_report(y_true, y_pred):
    report_dict = classification_report(y_true, y_pred, output_dict=True)
    df = pd.DataFrame(report_dict).transpose()
    return df
def evaluate_models(X_train, X_test, y_train, y_test):
    models =get_model_configs()
    
    results = {}

    plt.figure(figsize=(10, 6))
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average=[None, 'micro', 'macro', 'weighted'])
        recall = recall_score(y_test, y_pred, average=[None, 'micro', 'macro', 'weighted'])
        f1 = f1_score(y_test, y_pred, average=[None, 'micro', 'macro', 'weighte'])
        roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None
        
        results[name] = {
            "Accuracy": accuracy,
            "Precision": precision,
            "Recall": recall,
            "F1-score": f1,
            "ROC-AUC": roc_auc
        }

        if y_prob is not None:
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

    plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves")
    plt.legend()
    plt.show()
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    for ax, (name, model) in zip(axes.ravel(), models.items()):
        y_pred = model.predict(X_test)
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
        ax.set_title(f"{name} - Confusion Matrix")
        ax.set_xlabel("Predicted Label")
        ax.set_ylabel("True Label")
    
    plt.tight_layout()
    plt.show()

    results_df = pd.DataFrame(results).T
    results_df.plot(kind="bar", figsize=(10, 6))
    plt.title("Model Comparison")
    plt.ylabel("Score")
    plt.xticks(rotation=45)
    plt.legend(title="Metrics")
    plt.show()
    
    return results_df

def main():
    st.title("ðŸ¤–  Machine Learning Model Deployment")
    
    st.sidebar.header("Navigation")
    page = st.sidebar.radio("Go to", ["Home","Data Upload & Analysis", "Model Training","Visualisation", "Prediction"])
    
    if 'data' not in st.session_state:
        st.session_state.data = None
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None
    if 'label_encoders' not in st.session_state:
        st.session_state.label_encoders = None
    if 'model' not in st.session_state:
        st.session_state.model = None
    if 'features' not in st.session_state:
        st.session_state.features = None
    if 'target' not in st.session_state:
        st.session_state.target = None
    if 'model_name' not in st.session_state:
        st.session_state.model_name = None

    if page=="Home":
        st.title("ðŸš€ AutoML: Effortless Machine Learning")
        st.markdown(
        """
        Welcome to **AutoML**, a powerful yet easy-to-use tool that automates the process of building and evaluating 
        machine learning models. Whether you're a beginner exploring data or an expert looking for quick model deployment, 
        AutoML simplifies the entire workflow.
        """
        )

        st.header("ðŸ”¹ Features")
        st.markdown(
        """
        - **Automated Model Selection** â€“ Let AutoML pick the best algorithm for your data.
        - **Hyperparameter Tuning** â€“ Optimize model performance without manual tweaking.
        - **Data Preprocessing** â€“ Handle missing values, scaling, encoding, and feature engineering.
        - **Performance Evaluation** â€“ Compare models with key metrics and visualizations.
        - **Model Export** â€“ Save trained models for deployment.
        """
        )

        st.header("ðŸš€ Get Started")
        st.markdown(
        """
        1. **Upload your dataset** â€“ Provide a CSV or Excel file with your data.
        2. **Select your target variable** â€“ Choose the column to predict.
        3. **Let AutoML do the magic!** â€“ Sit back and watch the automation work.
        """
        )

        st.header("ðŸ“Š Visual Insights")
        st.markdown(
        """
        Explore interactive charts and performance metrics to make informed decisions. 
        Use visualizations to compare model accuracy, precision, recall, and other key statistics.
        """
        )

        st.success("Start automating your ML workflows now! ðŸŽ¯")
        st.write('''Developed By Gourav Singh,Ankit Yadav,Pushpansh''')
  
    if page == "Data Upload & Analysis":
        st.header("ðŸ“Š Data Upload & Analysis")
        
        uploaded_file = st.file_uploader("Upload your dataset (CSV or Excel)", type=['csv', 'xlsx', 'xls'])
        
        if uploaded_file is not None:
            st.session_state.data = load_data(uploaded_file)
            
            if st.session_state.data is not None:
                st.session_state.processed_data, st.session_state.label_encoders = auto_process_data(st.session_state.data)
                
                st.success("Data loaded and automatically processed!")
                
                st.subheader("Dataset Overview")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.info(f"Number of rows: {st.session_state.data.shape[0]}")
                with col2:
                    st.info(f"Number of columns: {st.session_state.data.shape[1]}")
                with col3:
                    missing_values = st.session_state.data.isnull().sum().sum()
                    st.info(f"Missing values: {missing_values} (Automatically handled)")
                
                st.subheader("Original Data Preview")
                st.dataframe(st.session_state.data.head())
                
                st.subheader("Processed Data Preview")
                st.dataframe(st.session_state.processed_data.head())
                
                st.subheader("Statistical Description")
                st.dataframe(st.session_state.processed_data.describe())
                
                st.subheader("Correlation Heatmap")
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.heatmap(st.session_state.processed_data.corr(), annot=True, cmap='coolwarm', ax=ax)
                st.pyplot(fig)
    
    elif page == "Model Training":
        st.header("ðŸŽ¯ Auto Model Training")
        
        if st.session_state.processed_data is None:
            st.warning("Please upload and process your data first!")
            return
            
        st.subheader("Select Features and Target")
        columns = st.session_state.processed_data.columns.tolist()
        
        st.session_state.features = st.multiselect("Select features", columns, default=columns[:-1])
        st.session_state.target = st.selectbox("Select target variable", columns)
        
        if st.button("Auto Train Models"):
            if len(st.session_state.features) > 0 and st.session_state.target:
                X = st.session_state.processed_data[st.session_state.features]
                y = st.session_state.processed_data[st.session_state.target]
                
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                
                st.session_state.model, st.session_state.model_name = auto_train(X_train, y_train, X_test, y_test)
                
                y_pred = st.session_state.model.predict(X_test)
                
                st.subheader("Best Model Performance")
                
                accuracy = accuracy_score(y_test, y_pred)
                st.metric("Accuracy", f"{accuracy:.2%}")
                
                st.text("Classification Report:")

                df_report = get_classification_report(y_test, y_pred)
                st.dataframe(df_report)
                
                if st.session_state.model_name == "Random Forest":
                    st.subheader("Feature Importance")
                    
                    importance_df = pd.DataFrame({
                        'Feature': st.session_state.features,
                        'Importance': st.session_state.model.named_steps['classifier'].feature_importances_
                    }).sort_values('Importance', ascending=False)
                    
                    fig = px.bar(importance_df, x='Feature', y='Importance',
                                title='Feature Importance Plot')
                    st.plotly_chart(fig)
                
                model_data = {
                    'model': st.session_state.model,
                    'model_name': st.session_state.model_name,
                    'label_encoders': st.session_state.label_encoders,
                    'features': st.session_state.features,
                    'target': st.session_state.target
                }
                joblib.dump(model_data, 'model_data.joblib')
                st.download_button(
                    label="Download trained model",
                    data=open('model_data.joblib', 'rb'),
                    file_name='model_data.joblib',
                    mime='application/octet-stream'
                )
    elif page=="Visualisation":
        st.header("Model Visualisation")
    if st.session_state.model is None:
        st.warning("Please train a model first!")
        return
    
    if st.session_state.processed_data is not None and st.session_state.features and st.session_state.target:
        X = st.session_state.processed_data[st.session_state.features]
        y = st.session_state.processed_data[st.session_state.target]
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Create visualization options
        viz_option = st.selectbox(
            "Select visualization type", 
            ["Model Comparison", "ROC Curves", "Confusion Matrix"]
        )
        
        if viz_option == "Model Comparison":
            st.subheader("Model Performance Metrics")
            
            # Train all models to compare
            models = get_model_configs()
            results = {}
            
            progress_bar = st.progress(0)
            progress_text = st.empty()
            
            for i, (name, model_config) in enumerate(models.items()):
                progress_text.text(f"Training {name}...")
                pipeline = model_config['pipeline']
                pipeline.fit(X_train, y_train)
                
                y_pred = pipeline.predict(X_test)
                y_prob = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None
                
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='binary')
                recall = recall_score(y_test, y_pred, average='binary')
                f1 = f1_score(y_test, y_pred, average='binary')
                roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None
                
                results[name] = {
                    "Accuracy": accuracy,
                    "Precision": precision,
                    "Recall": recall,
                    "F1-score": f1,
                    "ROC-AUC": roc_auc
                }
                
                progress_bar.progress((i + 1) / len(models))
            
            progress_text.empty()
            
            results_df = pd.DataFrame(results).T
            st.dataframe(results_df)
            
            fig = px.bar(
                results_df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Score'), 
                x='index', y='Score', color='Metric', 
                barmode='group',
                title='Model Comparison',
                labels={'index': 'Model'}
            )
            st.plotly_chart(fig)
            
        elif viz_option == "ROC Curves":
            st.subheader("ROC Curves")
            
            models = get_model_configs()
            
            fig = plt.figure(figsize=(10, 6))
            
            for name, model_config in models.items():
                pipeline = model_config['pipeline']
                pipeline.fit(X_train, y_train)
                
                if hasattr(pipeline, "predict_proba"):
                    y_prob = pipeline.predict_proba(X_test)[:, 1]
                    fpr, tpr, _ = roc_curve(y_test, y_prob)
                    roc_auc = auc(fpr, tpr)
                    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')
            
            plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title('Receiver Operating Characteristic (ROC) Curves')
            plt.legend(loc="lower right")
            
            st.pyplot(fig)
            
        elif viz_option == "Confusion Matrix":
            st.subheader("Confusion Matrices")
            
            models = get_model_configs()
            
            if len(models) > 4:
                st.warning("Showing confusion matrices for the first 4 models")
                model_items = list(models.items())[:4]
            else:
                model_items = list(models.items())
            
            num_models = len(model_items)
            cols = 2
            rows = (num_models + 1) // 2
            
            fig, axes = plt.subplots(rows, cols, figsize=(12, 10))
            axes = axes.flatten() if num_models > 1 else [axes]
            
            for i, (name, model_config) in enumerate(model_items):
                pipeline = model_config['pipeline']
                pipeline.fit(X_train, y_train)
                
                y_pred = pipeline.predict(X_test)
                cm = confusion_matrix(y_test, y_pred)
                
                sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[i])
                axes[i].set_title(f"{name} - Confusion Matrix")
                axes[i].set_xlabel("Predicted")
                axes[i].set_ylabel("Actual")
            
            for j in range(num_models, len(axes)):
                fig.delaxes(axes[j])
                
            plt.tight_layout()
            st.pyplot(fig)
            
        st.subheader("Current Model Performance")
        best_model_pred = st.session_state.model.predict(X_test)
        
        st.metric("Accuracy", f"{accuracy_score(y_test, best_model_pred):.2%}")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Precision", f"{precision_score(y_test, best_model_pred):.2%}")
            st.metric("F1 Score", f"{f1_score(y_test, best_model_pred):.2%}")
        with col2:
            st.metric("Recall", f"{recall_score(y_test, best_model_pred):.2%}")
            if hasattr(st.session_state.model, "predict_proba"):
                best_proba = st.session_state.model.predict_proba(X_test)[:, 1]
                st.metric("AUC", f"{roc_auc_score(y_test, best_proba):.2%}")
        
            else:
                st.warning("Please load and preprocess your dataset before running evaluation.")


    elif page == "Prediction":
        st.header("ðŸŽ² Make Predictions")
        
        if st.session_state.model is None:
            st.warning("Please train a model first!")
            return
            
        st.subheader("Enter Feature Values")
        st.info(f"Using best model: {st.session_state.model_name}")
        
        input_data = {}
        for feature in st.session_state.features:
            if feature in st.session_state.label_encoders:
                options = st.session_state.label_encoders[feature].classes_
                value = st.selectbox(f"Select {feature}", options)
                input_data[feature] = st.session_state.label_encoders[feature].transform([value])[0]
            else:
                input_data[feature] = st.number_input(f"Enter value for {feature}", value=0.0)
        if st.button("Predict"):
            input_df = pd.DataFrame([input_data])
            
            prediction = st.session_state.model.predict(input_df)
            
            if st.session_state.target in st.session_state.label_encoders:
                original_prediction = st.session_state.label_encoders[st.session_state.target].inverse_transform(prediction)
                st.success(f"Predicted {st.session_state.target}: {original_prediction[0]}")
            else:
                st.success(f"Predicted {st.session_state.target}: {prediction[0]}")
            
            proba = st.session_state.model.predict_proba(input_df)
            st.subheader("Prediction Probability")
            
            if st.session_state.target in st.session_state.label_encoders:
                classes = st.session_state.label_encoders[st.session_state.target].classes_
            else:
                classes = st.session_state.model.classes_
                
            proba_df = pd.DataFrame(
                proba,
                columns=classes
            )
            st.dataframe(proba_df)

if __name__ == "__main__":
    main()
